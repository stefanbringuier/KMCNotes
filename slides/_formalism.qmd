
## Monte Carlo Methods

* Solve complex problems using random sampling from a probablity distribution (i.e. stochastic description).
* Useful to evolve a physical system to a new state from an esemble of potential future states.

<!-- ![Monte Carlo methods @Andersen2019](figures/Andersen2019_fig1.jpg){#fig-andersent2019-1} -->


## Integrating a function MC sampling
::: {style="font-size: 0.66em; text-align: center"}

* If we want to evaluate the integral of a function over some domain we can numerically approximate this using the [midpoint rule](https://tinyurl.com/midpoint-numerical):
$$ \int_a^b f(x) dx = \frac{b-a}{N} \sum_{i=1}^N f(x_i) $$ {#eq-midpoint-rule}

* There is an alternative way to do this using probablity theory to determine the expectation value of a function $f(x)$ for random variable $x$:
$$ \int_a^b p(x) f(x) dx = \frac{b-a}{N} \sum_{i=1}^N f(x_i) $$ {#eq-mc-int}
where $p(x)$ is a uniform probablity distribution over the interval $[a,b]$.


* The difference between numerically evaluating @eq-midpoint-rule and @eq-mc-int, is that @eq-midpoint-rule is evaluated over a grid of points and @eq-mc-int is randomly sampled points.

* The error of MC integration is $\propto \frac{1}{\sqrt{N}}$ as a result of [central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem)

:::


```{julia}
#| echo: false
using Plots
using Distributions
begin
	struct Sampler{T<:Distribution,S<:Distribution}
		x::T
		y::S
	end
	Sampler() = Sampler([Uniform(),Uniform()])
	Base.rand(s::Sampler) = begin
		x′,y′ = rand(s.x), rand(s.y)
		return x′,y′
	end
end;

struct MCInt
	value::Number
	interval::Tuple{Number,Number}
	steps::Integer
	history::Vector
	points::AbstractArray
end;

function montecarlo_integrate(f::Function,a,b;
							  s::Sampler=Sampler(),
							  N=1000, ybounds=(0.0,1.0),
							  keep_points=false)
	count = 0.0e0;
	mcaccum = zeros(N)
	points = Tuple{Float64,Float64}[]
	h = sum(ybounds);
	for i=1:N
		x′,y′ = rand(s.x),rand(s.y)
		if y′ < f(x′)
			count += 1
		end
		keep_points ? push!(points,(x′,y′)) : nothing
		mcaccum[i] = (b-a) * h * count / i
	end
	value  = (b-a) * h * count / N
	return MCInt(value,(a,b),N,mcaccum,
			     reinterpret(reshape,Float64,points)
				)
	end
	
xₘᵢₙ,xₘₐₓ = 1.0, 4.0
analytical = (xₘₐₓ*log(xₘₐₓ)-xₘₐₓ) - (xₘᵢₙ*log(xₘᵢₙ)-xₘᵢₙ)

s = Sampler(Uniform(xₘᵢₙ,xₘₐₓ),Uniform(log(xₘᵢₙ),log(xₘₐₓ)))
mcint_log = montecarlo_integrate(log,xₘᵢₙ,xₘₐₓ;
								 s=s,
								 ybounds=(log(xₘᵢₙ),log(xₘₐₓ)),
 								 N=1000,
								 keep_points=true);
```

## Example integrating a function using MC sampling^[A more detailed notebook implementing the code can be viewed [here](https://stefanbringuier.github.io/PlutoNotebookBlogging/assets/notebooks/TrivialMC.html)]{auto-animate=true}


```{julia}
#| echo: false
#| label: fig-mc-int-a
#| fig-cap: Random sampled points from uniform distribution over the interval $[1,4]$. The black points are those that are accepted.
p1 = plot(log,xₘᵢₙ:0.1:xₘₐₓ,lw=3,lc=:blue,
	label="Function",xlabel="x",ylabel="log(x)")
z = map(x-> x ? :black : :red,log.(mcint_log.points[1,:]) .> mcint_log.points[2,:])  
scatter!(p1,mcint_log.points[1,:],mcint_log.points[2,:],label="Sampled points",mc=z,msc=nothing)
```

## Example integrating a function using MC sampling{auto-animate=true}

```{julia}
#| echo: false
#| label: fig-mc-int-b
#| fig-cap: Integration of $log(x)$ using MC.
p2 = plot(mcint_log.history,label="MC trajectory",
		xlabel="MC steps",ylabel="MC value")
plot!(p2,[analytical],st=:hline,lw=2,label="Analytical")
```

## Statistical Thermodynamics & Ensemble Properties {auto-animate=true}
::: {style="font-size: 0.7em; text-align: center"}

* Microscopic → Macroscopic description
	* How positions and momenta of $10^{23}$ particles relates to bulk temperature, pressure, or volume.
* Ensembles use probablity of specific microstate. Probability theory provides average of a function or variable, $\langle X \rangle$:
$$
\langle X \rangle = \frac{1}{N} \sum_{i=1}^N n_i \, X_i = \sum_{i=1}^N \underbrace{p_i}_{\text{PDF}} X_i
$${#eq-prob-averages}
* If $\langle X \rangle$ is continous, @eq-prob-averages is an integral. 
* $p_i$ is the probablity the system is in state $i$. The probablity density function (PDF) has the property that its normalized, i.e. $\sum_{i=1}^N p_i = 1$ 
:::

## Statistical Thermodynamics & Ensemble Properties {auto-animate=true}
::: {style="font-size: 0.7em; text-align: center"}
* The consqequence of @eq-prob-averages is that microscopic collections (i.e. ensemble of systems) can be used to calculate macroscopic properties.
* Choice of $p_i=\frac{z_i}{Z}$ depends on macroscopic conditions which manifest through the partition function:
$$
Z = \sum_i e^{-\beta\,X_i}
$$ {#eq-partition}
*  For a macroscopic system that has constant particles, volume and temperature, i.e., [canonical](https://en.wikipedia.org/wiki/Canonical_ensemble).
	* $\beta = \frac{1}{k_b\,T}$ and $X_i=E_i$ where Boltzmann factor is  $z_i = e^{-\frac{E_i}{k_b\,T}}$
$$
\langle E \rangle = \frac{1}{Z} \sum_i e^{-\frac{E_i}{k_b\,T}} E_i
$$ {#eq-energy-nvt}
:::

## Statistical Thermodynamics & Ensemble Properties {auto-animate=true}
::: {style="font-size: 0.7em; text-align: center"}

* The biggest challenge in evaluating @eq-energy-nvt is it requires knowledge of all possible configurations.
* If $Z$ is a configurational integral, e.g., $Z = \int e^{-U(\mathbf{r}^N)/k_B\,T} d\mathbf{r}^N$, then there are $3N$ possible configs!
* The key insight is that most configurations are not probable:
	- If the two atoms are extremely close at moderate $T$, the term $U(\mathbf{r})$ is large an hence the probablity low.
* The question then becomes, can we determine $p_i = \frac{1}{Z}e^{-\frac{E_i}{k_B\,T}}$ efficiently, that is the states with highest probablity centered around $\langle E \rangle$ given that $Z$ is not accessible.


::: aside
$U(\mathbf{r})$ is the potential energy between pairs of atoms.
:::

:::

## Metropolis Monte Carlo

* If we wanted to evaluate  @eq-energy-nvt  for an atomic system (i.e. the discrete states are replaced by continous atomic configurations), we could use the MC sampling as in @eq-mc-int.
* However we need to integrate over  $3N$ dimensions!
* This eliminates the feasability for determining the partition function $Z$ which is required to know the probablity of any specific configuration $p_i$

## Metropolis Monte Carlo {auto-animate=true}
::: {style="font-size: 0.9em; text-align: center"}

* The [Metropolis algorithm](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm) is a process to sample states $i$ with probablity $p_i$
* This is achieved by using relative probablities, i.e., $\frac{p_i}{p_j}$
* From this we get the correct average quantities.
* This works because, even though we don't know $Z$ and can't determine $p_i$, the results of $\frac{p_i}{p_j}$ gives the correct distribution
* Relative probablities  are given as:
$$
\frac{p_i}{p_j} = \frac{e^{-E_i/\,k_B\,T}}{Z} \frac{Z}{e^{-E_j/\,k_B\,T}} = e^{-(E_i - E_j)/\,k_B\,T}
$${#eq-relative-probs}
* Which only depends on energy difference between states as shown in graph
:::

## Metropolis Monte Carlo {auto-animate=true}
```{julia}
#| echo: false
#| label: fig-relative-probs
#| fig-cap: Relative probablity for two states.
plot(exp,range(0,10,length=100),yaxis=:log,ylabel="relative prob.", xlabel = "\$\\Delta E\$")
plot!(exp,range(-10,0,length=100),yaxis=:log)
hline!([1],label=nothing,ls=:dash,lc=:black)
annotate!([(-3,100,"if \$E_i - E_j \\leq 0\$ then \$\\frac{p_i}{p_j} \\geq 1\$")])
annotate!([(3,0.001,"if \$E_i - E_j > 0\$ then \$\\frac{p_i}{p_j}  < 1\$")])
```


## Metropolis Monte Carlo {auto-animate=true}
*  In the metropolis MC approach we use the relations on @fig-relative-probs to create a trajectory of states.
* The steps for MMC are:
    1. Generate configuration $i$ with $E_i$
	2. Randomly trial configuration, $i+1$, and calculate $E_{i+1}$
	3. Get relative probablity via @eq-relative-probs.
	4. Use relations in @fig-relative-probs to accept or accept with probability $\frac{p_i}{p_j} < 1$ given a randomly generated number betwen $(0,1)$
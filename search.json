[
  {
    "objectID": "presentation.html#monte-carlo-methods",
    "href": "presentation.html#monte-carlo-methods",
    "title": "Kinetic Monte Carlo Notes",
    "section": "Monte Carlo Methods",
    "text": "Monte Carlo Methods\n\nSolve complex problems using random sampling from a probablity distribution (i.e. stochastic description).\nUseful to evolve a physical system to a new state from an esemble of potential future states."
  },
  {
    "objectID": "presentation.html#integrating-a-function-mc-sampling",
    "href": "presentation.html#integrating-a-function-mc-sampling",
    "title": "Kinetic Monte Carlo Notes",
    "section": "Integrating a function MC sampling",
    "text": "Integrating a function MC sampling\n\n\nIf we want to evaluate the integral of a function over some domain we can numerically approximate this using the midpoint rule: \\[ \\int_a^b f(x) dx = \\frac{b-a}{N} \\sum_{i=1}^N f(x_i)  \\qquad(1)\\]\nThere is an alternative way to do this using probablity theory to determine the expectation value of a function \\(f(x)\\) for random variable \\(x\\): \\[ \\int_a^b p(x) f(x) dx = \\frac{b-a}{N} \\sum_{i=1}^N f(x_i)  \\qquad(2)\\] where \\(p(x)\\) is a uniform probablity distribution over the interval \\([a,b]\\).\nThe difference between numerically evaluating Equation 1 and Equation 2, is that Equation 1 is evaluated over a grid of points and Equation 2 is randomly sampled points.\nThe error of MC integration is \\(\\propto \\frac{1}{\\sqrt{N}}\\) as a result of central limit theorem"
  },
  {
    "objectID": "presentation.html#example-integrating-a-function-using-mc-sampling",
    "href": "presentation.html#example-integrating-a-function-using-mc-sampling",
    "title": "Kinetic Monte Carlo Notes",
    "section": "Example integrating a function using MC sampling1",
    "text": "Example integrating a function using MC sampling1\n\n\n\n\n\nFigure 1: Random sampled points from uniform distribution over the interval \\([1,4]\\). The black points are those that are accepted."
  },
  {
    "objectID": "presentation.html#example-integrating-a-function-using-mc-sampling-1",
    "href": "presentation.html#example-integrating-a-function-using-mc-sampling-1",
    "title": "Kinetic Monte Carlo Notes",
    "section": "Example integrating a function using MC sampling",
    "text": "Example integrating a function using MC sampling\n\nFigure 2: Integration of \\(log(x)\\) using MC."
  },
  {
    "objectID": "presentation.html#statistical-thermodynamics-ensemble-properties",
    "href": "presentation.html#statistical-thermodynamics-ensemble-properties",
    "title": "Kinetic Monte Carlo Notes",
    "section": "Statistical Thermodynamics & Ensemble Properties",
    "text": "Statistical Thermodynamics & Ensemble Properties\n\n\nMicroscopic → Macroscopic description\n\nHow positions and momenta of \\(10^{23}\\) particles relates to bulk temperature, pressure, or volume.\n\nEnsembles use probablity of specific microstate. Probability theory provides average of a function or variable, \\(\\langle X \\rangle\\): \\[\n\\langle X \\rangle = \\frac{1}{N} \\sum_{i=1}^N n_i \\, X_i = \\sum_{i=1}^N \\underbrace{p_i}_{\\text{PDF}} X_i\n\\qquad(3)\\]\nIf \\(\\langle X \\rangle\\) is continous, Equation 3 is an integral.\n\\(p_i\\) is the probablity the system is in state \\(i\\). The probablity density function (PDF) has the property that its normalized, i.e. \\(\\sum_{i=1}^N p_i = 1\\)"
  },
  {
    "objectID": "presentation.html#statistical-thermodynamics-ensemble-properties-1",
    "href": "presentation.html#statistical-thermodynamics-ensemble-properties-1",
    "title": "Kinetic Monte Carlo Notes",
    "section": "Statistical Thermodynamics & Ensemble Properties",
    "text": "Statistical Thermodynamics & Ensemble Properties\n\n\nThe consqequence of Equation 3 is that microscopic collections (i.e. ensemble of systems) can be used to calculate macroscopic properties.\nChoice of \\(p_i=\\frac{z_i}{Z}\\) depends on macroscopic conditions which manifest through the partition function: \\[\nZ = \\sum_i e^{-\\beta\\,X_i}\n\\qquad(4)\\]\nFor a macroscopic system that has constant particles, volume and temperature, i.e., canonical.\n\n\\(\\beta = \\frac{1}{k_b\\,T}\\) and \\(X_i=E_i\\) where Boltzmann factor is \\(z_i = e^{-\\frac{E_i}{k_b\\,T}}\\) \\[\n\\langle E \\rangle = \\frac{1}{Z} \\sum_i e^{-\\frac{E_i}{k_b\\,T}} E_i\n\\qquad(5)\\]"
  },
  {
    "objectID": "presentation.html#statistical-thermodynamics-ensemble-properties-2",
    "href": "presentation.html#statistical-thermodynamics-ensemble-properties-2",
    "title": "Kinetic Monte Carlo Notes",
    "section": "Statistical Thermodynamics & Ensemble Properties",
    "text": "Statistical Thermodynamics & Ensemble Properties\n\n\nThe biggest challenge in evaluating Equation 5 is it requires knowledge of all possible configurations.\nIf \\(Z\\) is a configurational integral, e.g., \\(Z = \\int e^{-U(\\mathbf{r}^N)/k_B\\,T} d\\mathbf{r}^N\\), then there are \\(3N\\) possible configs!\nThe key insight is that most configurations are not probable:\n\nIf the two atoms are extremely close at moderate \\(T\\), the term \\(U(\\mathbf{r})\\) is large an hence the probablity low.\n\nThe question then becomes, can we determine \\(p_i = \\frac{1}{Z}e^{-\\frac{E_i}{k_B\\,T}}\\) efficiently, that is the states with highest probablity centered around \\(\\langle E \\rangle\\) given that \\(Z\\) is not accessible.\n\n\n\n\n\\(U(\\mathbf{r})\\) is the potential energy between pairs of atoms."
  },
  {
    "objectID": "presentation.html#metropolis-monte-carlo",
    "href": "presentation.html#metropolis-monte-carlo",
    "title": "Kinetic Monte Carlo Notes",
    "section": "Metropolis Monte Carlo",
    "text": "Metropolis Monte Carlo\n\nIf we wanted to evaluate Equation 5 for an atomic system (i.e. the discrete states are replaced by continous atomic configurations), we could use the MC sampling as in Equation 2.\nHowever we need to integrate over \\(3N\\) dimensions!\nThis eliminates the feasability for determining the partition function \\(Z\\) which is required to know the probablity of any specific configuration \\(p_i\\)"
  },
  {
    "objectID": "presentation.html#metropolis-monte-carlo-1",
    "href": "presentation.html#metropolis-monte-carlo-1",
    "title": "Kinetic Monte Carlo Notes",
    "section": "Metropolis Monte Carlo",
    "text": "Metropolis Monte Carlo\n\n\nThe Metropolis algorithm is a process to sample states \\(i\\) with probablity \\(p_i\\)\nThis is achieved by using relative probablities, i.e., \\(\\frac{p_i}{p_j}\\)\nFrom this we get the correct average quantities.\nThis works because, even though we don’t know \\(Z\\) and can’t determine \\(p_i\\), the results of \\(\\frac{p_i}{p_j}\\) gives the correct distribution\nRelative probablities are given as: \\[\n\\frac{p_i}{p_j} = \\frac{e^{-E_i/\\,k_B\\,T}}{Z} \\frac{Z}{e^{-E_j/\\,k_B\\,T}} = e^{-(E_i - E_j)/\\,k_B\\,T}\n\\qquad(6)\\]\nWhich only depends on energy difference between states as shown in graph"
  },
  {
    "objectID": "presentation.html#metropolis-monte-carlo-2",
    "href": "presentation.html#metropolis-monte-carlo-2",
    "title": "Kinetic Monte Carlo Notes",
    "section": "Metropolis Monte Carlo",
    "text": "Metropolis Monte Carlo\n\nFigure 3: Relative probablity for two states."
  },
  {
    "objectID": "presentation.html#metropolis-monte-carlo-3",
    "href": "presentation.html#metropolis-monte-carlo-3",
    "title": "Kinetic Monte Carlo Notes",
    "section": "Metropolis Monte Carlo",
    "text": "Metropolis Monte Carlo\n\nIn the metropolis MC approach we use the relations on Figure 3 to create a trajectory of states.\nThe steps for MMC are:\n\nGenerate configuration \\(i\\) with \\(E_i\\)\nRandomly trial configuration, \\(i+1\\), and calculate \\(E_{i+1}\\)\nGet relative probablity via Equation 6.\nUse relations in Figure 3 to accept or accept with probability \\(\\frac{p_i}{p_j} < 1\\) given a randomly generated number betwen \\((0,1)\\)\nIf accepted, add \\(i+1\\) to trajectory, otherwise add \\(i\\) again2. Repeat until quantity \\(\\langle X \\rangle = \\frac{1}{N} \\sum_{i=1}^N X_i\\)"
  },
  {
    "objectID": "presentation.html#metropolis-monte-carlo-4",
    "href": "presentation.html#metropolis-monte-carlo-4",
    "title": "Kinetic Monte Carlo Notes",
    "section": "Metropolis Monte Carlo",
    "text": "Metropolis Monte Carlo\n\nFigure 4: Acceptance and rejection of Metropolis MC (see LeSar (2013))."
  },
  {
    "objectID": "presentation.html#backmatter",
    "href": "presentation.html#backmatter",
    "title": "Kinetic Monte Carlo Notes",
    "section": "Backmatter",
    "text": "Backmatter\nConnect with me!\n\n\n\n\n\n\nstefanbringuier@gmail.com\n  \n\n\n\n\n\n\n\nNote\n\n\nThis presentation can be viewed online at https://stefanbringuier.github.io/KMCNotes. A report formated PDF of this presentation can be downloaded here.\n\n\n\n\n\n\n\n\n\nTip\n\n\nTo export revealjs presentations to pdf, press ‘e’ then ‘ctrl-p’ ➡ ‘save as pdf’"
  },
  {
    "objectID": "presentation.html#references",
    "href": "presentation.html#references",
    "title": "Kinetic Monte Carlo Notes",
    "section": "References",
    "text": "References\n\n\nAndersen, Mie, Chiara Panosetti, and Karsten Reuter. 2019. “A Practical Guide to Surface Kinetic Monte Carlo Simulations.” Frontiers in Chemistry 7. https://doi.org/10.3389/fchem.2019.00202.\n\n\nLeSar, R. 2013. Introduction to Computational Materials Science: Fundamentals to Applications. Cambridge University Press. https://books.google.com/books?id=QzkhAwAAQBAJ.\n\n\n“Numerical Integration - Midpoint, Trapezoid, Simpson’s Rule.” 2021.\n\n\n\n\n\nStefan Bringuier"
  }
]